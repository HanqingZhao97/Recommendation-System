{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hanqing Zhao\n",
    "#### CSE258 Midterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import gzip\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will suppress any warnings, comment out if you'd like to preserve them\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check formatting of submissions\n",
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"spoilers.json.gz\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for l in f:\n",
    "    d = eval(l)\n",
    "    dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few utility data structures\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "\n",
    "for d in dataset:\n",
    "    u,i = d['user_id'],d['book_id']\n",
    "    reviewsPerUser[u].append(d)\n",
    "    reviewsPerItem[i].append(d)\n",
    "\n",
    "# Sort reviews per user by timestamp\n",
    "for u in reviewsPerUser:\n",
    "    reviewsPerUser[u].sort(key=lambda x: x['timestamp'])\n",
    "    \n",
    "# Same for reviews per item\n",
    "for i in reviewsPerItem:\n",
    "    reviewsPerItem[i].sort(key=lambda x: x['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2012-03-13',\n",
       " '2013-05-06',\n",
       " '2013-09-03',\n",
       " '2015-04-05',\n",
       " '2016-02-10',\n",
       " '2016-05-29']"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E.g. reviews for this user are sorted from earliest to most recent\n",
    "[d['timestamp'] for d in reviewsPerUser['b0d7e561ca59e313b728dc30a5b1862e']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    return mean_squared_error(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "ypred = []\n",
    "\n",
    "for u in reviewsPerUser:\n",
    "    if len(reviewsPerUser[u]) > 1:\n",
    "        y.append(reviewsPerUser[u][-1]['rating'])\n",
    "        total = sum(reviewsPerUser[u][i]['rating'] for i in range(len(reviewsPerUser[u])-1))\n",
    "        average = total/(len(reviewsPerUser[u])-1)\n",
    "        ypred.append(average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9704162943957526\n"
     ]
    }
   ],
   "source": [
    "answers['Q1a'] = MSE(y,ypred)\n",
    "print(answers['Q1a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q1a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = []\n",
    "ypred_ = []\n",
    "\n",
    "for i in reviewsPerItem:\n",
    "    if len(reviewsPerItem[i]) > 1:\n",
    "        y_.append(reviewsPerItem[i][-1]['rating'])\n",
    "        total = sum(reviewsPerItem[i][x]['rating'] for x in range(len(reviewsPerItem[i][:-1])))\n",
    "        average = total/len(reviewsPerItem[i][:-1])\n",
    "        ypred_.append(average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.05196610339507\n"
     ]
    }
   ],
   "source": [
    "answers['Q1b'] = MSE(y_,ypred_)\n",
    "print(answers['Q1b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q1b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_N_ratings(N, data):\n",
    "    y = []\n",
    "    ypred = []\n",
    "\n",
    "    for u in data:\n",
    "        if len(data[u]) > N+1:\n",
    "            y.append(data[u][-1]['rating'])\n",
    "            total = sum(data[u][i]['rating'] for i in range(len(data[u][-(N+1):-1])))\n",
    "            average = total/(len(reviewsPerUser[u][-(N+1):-1]))\n",
    "            ypred.append(average)\n",
    "        elif len(data[u]) <= N+1 and len(data[u]) > 1:\n",
    "            y.append(data[u][-1]['rating'])\n",
    "            total = sum(data[u][i]['rating'] for i in range(len(data[u][:-1])))\n",
    "            average = total/(len(reviewsPerUser[u][:-1]))\n",
    "            ypred.append(average)\n",
    "    return y, ypred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q2'] = []\n",
    "\n",
    "for N in [1,2,3]:\n",
    "    y,ypred = mean_N_ratings(N,reviewsPerUser)\n",
    "    answers['Q2'].append(MSE(y,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6814096499526965, 2.187263481551561, 2.0563439503836856]\n"
     ]
    }
   ],
   "source": [
    "print(answers['Q2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q2'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature3(N, u): # For a user u and a window size of N\n",
    "    output = [1]\n",
    "    ratings = []\n",
    "    for d in reviewsPerUser[u][-(N+1):-1]:\n",
    "        ratings.append(d['rating'])\n",
    "    output = output+ratings[::-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3a'] = [feature3(2,dataset[0]['user_id']), feature3(3,dataset[0]['user_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 4, 4], [1, 4, 4, 4]]"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q3a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q3a']) == 2\n",
    "assert len(answers['Q3a'][0]) == 3\n",
    "assert len(answers['Q3a'][1]) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3b'] = []\n",
    "\n",
    "for N in [1,2,3]:\n",
    "    X = []\n",
    "    Y = []\n",
    "    for u in reviewsPerUser:\n",
    "        if len(reviewsPerUser[u]) >= N+1:\n",
    "            X.append(feature3(N, u))\n",
    "            Y.append(reviewsPerUser[u][-1]['rating'])\n",
    "        \n",
    "    theta, residuals, rank, s = np.linalg.lstsq(X,Y)\n",
    "    ypred = np.dot(X,theta)\n",
    "    mse = MSE(Y,ypred)\n",
    "    answers['Q3b'].append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5608319121482543, 1.5409512373315701, 1.5396484853948416]"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q3b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q3b'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalAverage = [d['rating'] for d in dataset]\n",
    "globalAverage = sum(globalAverage) / len(globalAverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMeanValue(N, u): # For a user u and a window size of N\n",
    "    output = [1]\n",
    "    ratings = []\n",
    "    #append global average\n",
    "    if len(reviewsPerUser[u]) <= 1:\n",
    "        for i in range(N):\n",
    "            output.append(globalAverage)\n",
    "    #append list's average\n",
    "    elif len(reviewsPerUser[u]) < N+1:\n",
    "        for d in reviewsPerUser[u][:-1]:\n",
    "            ratings.append(d['rating'])\n",
    "        output = output+ratings[::-1]\n",
    "        ave = sum(ratings)/len(ratings)\n",
    "        for i in range(len(reviewsPerUser[u])-1,N):\n",
    "            output.append(ave)\n",
    "    #append no average\n",
    "    else:\n",
    "        for d in reviewsPerUser[u][-(N+1):-1]:\n",
    "            ratings.append(d['rating'])\n",
    "        output = output+ratings[::-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMissingValue(N, u):\n",
    "    output = [1]\n",
    "    ratings = []\n",
    "    #append 1，0\n",
    "    if len(reviewsPerUser[u]) < N+1:\n",
    "        for d in reviewsPerUser[u][:-1]:\n",
    "            ratings.append(d['rating'])\n",
    "            ratings.append(0)\n",
    "        output = output+ratings[::-1]\n",
    "        for i in range(len(reviewsPerUser[u])-1,N):\n",
    "            output += [1,0]\n",
    "    #append only 0,x\n",
    "    else:\n",
    "        for d in reviewsPerUser[u][-(N+1):-1]:\n",
    "            ratings.append(d['rating'])\n",
    "            ratings.append(0)\n",
    "        output = output+ratings[::-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q4a'] = [featureMeanValue(10, dataset[0]['user_id']), featureMissingValue(10, dataset[0]['user_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q4a']) == 2\n",
    "assert len(answers['Q4a'][0]) == 11\n",
    "assert len(answers['Q4a'][1]) == 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q4b'] = []\n",
    "\n",
    "for featFunc in [featureMeanValue, featureMissingValue]:\n",
    "    X = []\n",
    "    Y = []\n",
    "    for u in reviewsPerUser:\n",
    "        X.append(featFunc(10, u))\n",
    "        Y.append(reviewsPerUser[u][-1]['rating'])\n",
    "        \n",
    "    theta, residuals, rank, s = np.linalg.lstsq(X,Y)\n",
    "    ypred = np.dot(X,theta)\n",
    "    mse = MSE(Y,ypred)\n",
    "\n",
    "    answers['Q4b'].append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5480131297119455, 1.5354064599216093]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q4b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers[\"Q4b\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature5(sentence):\n",
    "    output = [1]\n",
    "    output.append(len(sentence))\n",
    "    output.append(sentence.count('!'))\n",
    "    output.append(len(re.findall(r'[A-Z]',sentence)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "X = []\n",
    "\n",
    "for d in dataset:\n",
    "    for spoiler,sentence in d['review_sentences']:\n",
    "        X.append(feature5(sentence))\n",
    "        y.append(spoiler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight='balanced')"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = linear_model.LogisticRegression(fit_intercept=True, class_weight='balanced', C=1)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BER(predictions, y):\n",
    "    TP_ = np.logical_and(predictions, y)\n",
    "    FP_ = np.logical_and(predictions, np.logical_not(y))\n",
    "    TN_ = np.logical_and(np.logical_not(predictions), np.logical_not(y))\n",
    "    FN_ = np.logical_and(np.logical_not(predictions), y)\n",
    "    \n",
    "    TP = sum(TP_)\n",
    "    FP = sum(FP_)\n",
    "    TN = sum(TN_)\n",
    "    FN = sum(FN_)\n",
    "    \n",
    "    BER = 1 - 0.5*(TP / (TP + FN) + TN / (TN + FP))\n",
    "    return TP,TN,FP,FN,BER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, TN, FP, FN, BER = get_BER(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5a'] = X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5b'] = [TP, TN, FP, FN, BER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q5a']) == 4\n",
    "assertFloatList(answers['Q5b'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature6(review):\n",
    "    output = []\n",
    "    for i in range(5):\n",
    "        #get the spoiler labels\n",
    "        output.append(review['review_sentences'][i][0])\n",
    "    output += feature5(review['review_sentences'][5][1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "X = []\n",
    "\n",
    "for d in dataset:\n",
    "    sentences = d['review_sentences']\n",
    "    if len(sentences) >= 6:\n",
    "        X.append(feature6(d))\n",
    "        y.append(sentences[5][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(fit_intercept=True, class_weight='balanced', C=1)\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "TP, TN, FP, FN, BER = get_BER(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6a'] = X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6b'] = BER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q6a']) == 9\n",
    "assertFloat(answers['Q6b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50/25/25% train/valid/test split\n",
    "Xtrain, Xvalid, Xtest = X[:len(X)//2], X[len(X)//2:(3*len(X))//4], X[(3*len(X))//4:]\n",
    "ytrain, yvalid, ytest = y[:len(X)//2], y[len(X)//2:(3*len(X))//4], y[(3*len(X))//4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BER(predictions, y):\n",
    "    TP_ = np.logical_and(predictions, y)\n",
    "    FP_ = np.logical_and(predictions, np.logical_not(y))\n",
    "    TN_ = np.logical_and(np.logical_not(predictions), np.logical_not(y))\n",
    "    FN_ = np.logical_and(np.logical_not(predictions), y)\n",
    "    \n",
    "    TP = sum(TP_)\n",
    "    FP = sum(FP_)\n",
    "    TN = sum(TN_)\n",
    "    FN = sum(FN_)\n",
    "    \n",
    "    BER = 1 - 0.5*(TP / (TP + FN) + TN / (TN + FP))\n",
    "    return BER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "bers = []\n",
    "bestC = 0\n",
    "minBer = 10000\n",
    "for c in [0.01, 0.1, 1, 10, 100]:\n",
    "    mod = linear_model.LogisticRegression(C=c, class_weight='balanced')\n",
    "    mod.fit(Xtrain,ytrain)\n",
    "    pred_valid = mod.predict(Xvalid)\n",
    "    berValid = BER(pred_valid, yvalid)\n",
    "    bers.append(berValid)\n",
    "    if berValid < minBer:\n",
    "        minBer = berValid\n",
    "        bestC = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best c for test\n",
    "mod_best = linear_model.LogisticRegression(C=bestC, class_weight='balanced')\n",
    "mod_best.fit(Xtrain,ytrain)\n",
    "\n",
    "pred_test = mod_best.predict(Xtest)\n",
    "ber = BER(pred_test, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q7'] = bers + [bestC] + [ber]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q7'], 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75/25% train/test split\n",
    "dataTrain = dataset[:15000]\n",
    "dataTest = dataset[15000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few utilities\n",
    "\n",
    "itemAverages = defaultdict(list)\n",
    "userAverages = defaultdict(list)\n",
    "ratingMean = []\n",
    "\n",
    "for d in dataTrain:\n",
    "    itemAverages[d['book_id']].append(d['rating'])\n",
    "    ratingMean.append(d['rating'])\n",
    "    userAverages[d['user_id']].append(d['rating'])\n",
    "\n",
    "for i in itemAverages:\n",
    "    itemAverages[i] = sum(itemAverages[i]) / len(itemAverages[i])\n",
    "\n",
    "for u in userAverages:\n",
    "    userAverages[u] = sum(userAverages[u]) / len(userAverages[u])\n",
    "\n",
    "ratingMean = sum(ratingMean) / len(ratingMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsPerUser = defaultdict(list)\n",
    "usersPerItem = defaultdict(set)\n",
    "ratingDict = defaultdict(list)\n",
    "for d in dataTrain:\n",
    "    u,i = d['user_id'], d['book_id']\n",
    "    reviewsPerUser[u].append(d)\n",
    "    usersPerItem[i].add(u)\n",
    "    ratingDict[(u,i)] = d['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From my HW2 solution, welcome to reuse\n",
    "def predictRating(user,item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        i2 = d['book_id']\n",
    "        if i2 == item: continue\n",
    "        ratings.append(d['rating'] - itemAverages[i2])\n",
    "        similarities.append(Jaccard(usersPerItem[item],usersPerItem[i2]))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return itemAverages[item] + sum(weightedRatings) / sum(similarities)\n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        if item in itemAverages:\n",
    "            return itemAverages[item]\n",
    "        else:\n",
    "            return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [predictRating(d['user_id'], d['book_id']) for d in dataTest]\n",
    "labels = [d['rating'] for d in dataTest]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.816493441279133\n"
     ]
    }
   ],
   "source": [
    "answers[\"Q8\"] = MSE(predictions, labels)\n",
    "print(answers[\"Q8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers[\"Q8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = []\n",
    "data_1to5 = []\n",
    "data_5 = []\n",
    "\n",
    "def count_occur(dataTrain, book_id):\n",
    "    occur = 0\n",
    "    for d in dataTrain:\n",
    "        if d['book_id'] == book_id:\n",
    "            occur += 1\n",
    "    return occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataTest:\n",
    "    count = count_occur(dataTrain, d['book_id'])\n",
    "    if count == 0:\n",
    "        data_0.append(d)\n",
    "    elif count >=1 and count <= 5:\n",
    "        data_1to5.append(d)\n",
    "    elif count > 5:\n",
    "        data_5.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred0 = [predictRating(d['user_id'], d['book_id']) for d in data_0]\n",
    "pred1to5 = [predictRating(d['user_id'], d['book_id']) for d in data_1to5]\n",
    "pred5 = [predictRating(d['user_id'], d['book_id']) for d in data_5]\n",
    "\n",
    "labels0 = [d['rating'] for d in data_0]\n",
    "labels1to5 = [d['rating'] for d in data_1to5]\n",
    "labels5 = [d['rating'] for d in data_5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse0 = MSE(pred0, labels0)\n",
    "mse1to5 = MSE(pred1to5, labels1to5)\n",
    "mse5 = MSE(pred5, labels5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.7420124844444445, 2.0526818720058895, 1.4520632348645055]\n"
     ]
    }
   ],
   "source": [
    "answers[\"Q9\"] = [mse0, mse1to5, mse5]\n",
    "print(answers[\"Q9\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers[\"Q9\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRating2(user,item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        i2 = d['book_id']\n",
    "        if i2 == item: continue\n",
    "        ratings.append(d['rating'] - itemAverages[i2])\n",
    "        similarities.append(Jaccard(usersPerItem[item],usersPerItem[i2]))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return itemAverages[item] + sum(weightedRatings) / sum(similarities)\n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        if item in itemAverages:\n",
    "            return itemAverages[item]\n",
    "        else:\n",
    "            if user in userAverages:\n",
    "                return userAverages[user]\n",
    "            else:\n",
    "                return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred0_ = [predictRating2(d['user_id'], d['book_id']) for d in data_0]\n",
    "labels0_ = [d['rating'] for d in data_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6696633366192306\n"
     ]
    }
   ],
   "source": [
    "itsMSE = MSE(pred0_, labels0_)\n",
    "print(itsMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers[\"Q10\"] = (\"Since for the data that are not in the training set, we always return the global ratingMean because their similarities are always zero. I changed the else condition, which is when the similarities are 0, to return the average rating that a specific user will gave, to introduce more reasonable variances based on the user's rating histories and thus could reduce the MSE a little bit.\", itsMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(answers[\"Q10\"][0]) == str\n",
    "assertFloat(answers[\"Q10\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_midterm.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
